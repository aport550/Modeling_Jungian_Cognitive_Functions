{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/allanporter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sb\n",
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mbti_1.csv')\n",
    "count = Counter(df)\n",
    "X = pd.DataFrame.from_dict(count, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get The characters per line\n",
    "import string\n",
    "[line.rstrip() for line in df['posts'][0]]\n",
    "c = [c for c in df['posts'][0] if c not in string.punctuation]\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ''.join(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'httpwwwyoutubecomwatchvqsXHcwe3krwhttp41mediatumblrcomtumblrlfouy03PMA1qa1rooo1500jpgenfp and intj moments  httpswwwyoutubecomwatchviz7lE1g4XM4  sportscenter not top ten plays  httpswwwyoutubecomwatchvuCdfze1etec  pranksWhat has been the most lifechanging experience in your lifehttpwwwyoutubecomwatchvvXZeYwwRDw8   httpwwwyoutubecomwatchvu8ejam5DP3E  On repeat for most of todayMay the PerC Experience immerse youThe last thing my INFJ friend posted on his facebook before committing suicide the next day Rest in peace   httpvimeocom22842206Hello ENFJ7 Sorry to hear of your distress Its only natural for a relationship to not be perfection all the time in every moment of existence Try to figure the hard times as times of growth as84389  84390  httpwallpaperpassioncomupload23700friendshipboyandgirlwallpaperjpg  httpassetsdornobcomwpcontentuploads201004roundhomedesignjpg Welcome and stuffhttpplayeressencecomwpcontentuploads201308REDredthepokemonmaster32560474450338jpg  Game Set MatchProzac wellbrutin at least thirty minutes of moving your legs and I dont mean moving them while sitting in your same desk chair weed in moderation maybe try edibles as a healthier alternativeBasically come up with three items youve determined that each type or whichever types you want to do would more than likely use given each types cognitive functions and whatnot when left byAll things in moderation  Sims is indeed a video game and a good one at that Note a good one at that is somewhat subjective in that I am not completely promoting the death of any given SimDear ENFP  What were your favorite video games growing up and what are your now current favorite video games coolhttpswwwyoutubecomwatchvQyPqT8umzmYIt appears to be too late sadTheres someone out there for everyoneWait I thought confidence was a good thingI just cherish the time of solitude bc i revel within my inner world more whereas most other time id be workin just enjoy the me time while you can Dont worry people will always be around toYo entp ladies if youre into a complimentary personalitywell hey when your main social outlet is xbox live conversations and even then you verbally fatigue quicklyhttpwwwyoutubecomwatchvgDhy7rdfm14  I really dig the part from 146 to 250httpwwwyoutubecomwatchvmsqXffgh7b8Banned because this thread requires it of meGet high in backyard roast and eat marshmellows in backyard while conversing over something intellectual followed by massages and kisseshttpwwwyoutubecomwatchvMw7eoU3BMbEhttpwwwyoutubecomwatchv4V2uYORhQOkhttpwwwyoutubecomwatchvSlVmgFQQ0TIBanned for too many bs in that sentence How could you Think of the BBanned for watching movies in the corner with the duncesBanned because Health class clearly taught you nothing about peer pressureBanned for a whole host of reasonshttpwwwyoutubecomwatchvIRcrv41hgz41 Two baby deer on left and right munching on a beetle in the middle  2 Using their own blood two cavemen diary todays latest happenings on their designated cave diary wall  3 I see it asa pokemon world  an infj society  everyone becomes an optimist49142httpwwwyoutubecomwatchvZRCEqJFeFMhttpdiscovermagazinecom2012julaug20thingsyoudidntknowaboutdesertsdesertjpghttpoysterignimgscommediawikiapisigncompokemonsilverversiondddDittogifhttpwwwserebiinetpotwdpScizorjpgNot all artists are artists because they draw Its the idea that counts in forming something of your own like a signatureWelcome to the robot ranks person who downed my selfesteem cuz Im not an avid signature artist like herself proudBanned for taking all the room under my bed Ya gotta learn to share with the roacheshttpwwwyoutubecomwatchvw8IgImn57aQBanned for being too much of a thundering grumbling kind of storm yepAhh old high school music I havent heard in ages   httpwwwyoutubecomwatchvdcCRUPCdB1wI failed a public speaking class a few years ago and Ive sort of learned what I could do better were I to be in that position again A big part of my failure was just overloading myself with tooI like this persons mentality Hes a confirmed INTJ by the way httpwwwyoutubecomwatchvhGKLIGEc6MMove to the Denver area and start a new life for myself'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.split()\n",
    "clean_mess = [word for word in c.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['httpwwwyoutubecomwatchvqsXHcwe3krwhttp41mediatumblrcomtumblrlfouy03PMA1qa1rooo1500jpgenfp',\n",
       " 'intj',\n",
       " 'moments',\n",
       " 'httpswwwyoutubecomwatchviz7lE1g4XM4',\n",
       " 'sportscenter',\n",
       " 'top',\n",
       " 'ten',\n",
       " 'plays',\n",
       " 'httpswwwyoutubecomwatchvuCdfze1etec',\n",
       " 'pranksWhat',\n",
       " 'lifechanging',\n",
       " 'experience',\n",
       " 'lifehttpwwwyoutubecomwatchvvXZeYwwRDw8',\n",
       " 'httpwwwyoutubecomwatchvu8ejam5DP3E',\n",
       " 'repeat',\n",
       " 'todayMay',\n",
       " 'PerC',\n",
       " 'Experience',\n",
       " 'immerse',\n",
       " 'youThe',\n",
       " 'last',\n",
       " 'thing',\n",
       " 'INFJ',\n",
       " 'friend',\n",
       " 'posted',\n",
       " 'facebook',\n",
       " 'committing',\n",
       " 'suicide',\n",
       " 'next',\n",
       " 'day',\n",
       " 'Rest',\n",
       " 'peace',\n",
       " 'httpvimeocom22842206Hello',\n",
       " 'ENFJ7',\n",
       " 'Sorry',\n",
       " 'hear',\n",
       " 'distress',\n",
       " 'natural',\n",
       " 'relationship',\n",
       " 'perfection',\n",
       " 'time',\n",
       " 'every',\n",
       " 'moment',\n",
       " 'existence',\n",
       " 'Try',\n",
       " 'figure',\n",
       " 'hard',\n",
       " 'times',\n",
       " 'times',\n",
       " 'growth',\n",
       " 'as84389',\n",
       " '84390',\n",
       " 'httpwallpaperpassioncomupload23700friendshipboyandgirlwallpaperjpg',\n",
       " 'httpassetsdornobcomwpcontentuploads201004roundhomedesignjpg',\n",
       " 'Welcome',\n",
       " 'stuffhttpplayeressencecomwpcontentuploads201308REDredthepokemonmaster32560474450338jpg',\n",
       " 'Game',\n",
       " 'Set',\n",
       " 'MatchProzac',\n",
       " 'wellbrutin',\n",
       " 'least',\n",
       " 'thirty',\n",
       " 'minutes',\n",
       " 'moving',\n",
       " 'legs',\n",
       " 'dont',\n",
       " 'mean',\n",
       " 'moving',\n",
       " 'sitting',\n",
       " 'desk',\n",
       " 'chair',\n",
       " 'weed',\n",
       " 'moderation',\n",
       " 'maybe',\n",
       " 'try',\n",
       " 'edibles',\n",
       " 'healthier',\n",
       " 'alternativeBasically',\n",
       " 'come',\n",
       " 'three',\n",
       " 'items',\n",
       " 'youve',\n",
       " 'determined',\n",
       " 'type',\n",
       " 'whichever',\n",
       " 'types',\n",
       " 'want',\n",
       " 'would',\n",
       " 'likely',\n",
       " 'use',\n",
       " 'given',\n",
       " 'types',\n",
       " 'cognitive',\n",
       " 'functions',\n",
       " 'whatnot',\n",
       " 'left',\n",
       " 'byAll',\n",
       " 'things',\n",
       " 'moderation',\n",
       " 'Sims',\n",
       " 'indeed',\n",
       " 'video',\n",
       " 'game',\n",
       " 'good',\n",
       " 'one',\n",
       " 'Note',\n",
       " 'good',\n",
       " 'one',\n",
       " 'somewhat',\n",
       " 'subjective',\n",
       " 'completely',\n",
       " 'promoting',\n",
       " 'death',\n",
       " 'given',\n",
       " 'SimDear',\n",
       " 'ENFP',\n",
       " 'favorite',\n",
       " 'video',\n",
       " 'games',\n",
       " 'growing',\n",
       " 'current',\n",
       " 'favorite',\n",
       " 'video',\n",
       " 'games',\n",
       " 'coolhttpswwwyoutubecomwatchvQyPqT8umzmYIt',\n",
       " 'appears',\n",
       " 'late',\n",
       " 'sadTheres',\n",
       " 'someone',\n",
       " 'everyoneWait',\n",
       " 'thought',\n",
       " 'confidence',\n",
       " 'good',\n",
       " 'thingI',\n",
       " 'cherish',\n",
       " 'time',\n",
       " 'solitude',\n",
       " 'bc',\n",
       " 'revel',\n",
       " 'within',\n",
       " 'inner',\n",
       " 'world',\n",
       " 'whereas',\n",
       " 'time',\n",
       " 'id',\n",
       " 'workin',\n",
       " 'enjoy',\n",
       " 'time',\n",
       " 'Dont',\n",
       " 'worry',\n",
       " 'people',\n",
       " 'always',\n",
       " 'around',\n",
       " 'toYo',\n",
       " 'entp',\n",
       " 'ladies',\n",
       " 'youre',\n",
       " 'complimentary',\n",
       " 'personalitywell',\n",
       " 'hey',\n",
       " 'main',\n",
       " 'social',\n",
       " 'outlet',\n",
       " 'xbox',\n",
       " 'live',\n",
       " 'conversations',\n",
       " 'even',\n",
       " 'verbally',\n",
       " 'fatigue',\n",
       " 'quicklyhttpwwwyoutubecomwatchvgDhy7rdfm14',\n",
       " 'really',\n",
       " 'dig',\n",
       " 'part',\n",
       " '146',\n",
       " '250httpwwwyoutubecomwatchvmsqXffgh7b8Banned',\n",
       " 'thread',\n",
       " 'requires',\n",
       " 'meGet',\n",
       " 'high',\n",
       " 'backyard',\n",
       " 'roast',\n",
       " 'eat',\n",
       " 'marshmellows',\n",
       " 'backyard',\n",
       " 'conversing',\n",
       " 'something',\n",
       " 'intellectual',\n",
       " 'followed',\n",
       " 'massages',\n",
       " 'kisseshttpwwwyoutubecomwatchvMw7eoU3BMbEhttpwwwyoutubecomwatchv4V2uYORhQOkhttpwwwyoutubecomwatchvSlVmgFQQ0TIBanned',\n",
       " 'many',\n",
       " 'bs',\n",
       " 'sentence',\n",
       " 'could',\n",
       " 'Think',\n",
       " 'BBanned',\n",
       " 'watching',\n",
       " 'movies',\n",
       " 'corner',\n",
       " 'duncesBanned',\n",
       " 'Health',\n",
       " 'class',\n",
       " 'clearly',\n",
       " 'taught',\n",
       " 'nothing',\n",
       " 'peer',\n",
       " 'pressureBanned',\n",
       " 'whole',\n",
       " 'host',\n",
       " 'reasonshttpwwwyoutubecomwatchvIRcrv41hgz41',\n",
       " 'Two',\n",
       " 'baby',\n",
       " 'deer',\n",
       " 'left',\n",
       " 'right',\n",
       " 'munching',\n",
       " 'beetle',\n",
       " 'middle',\n",
       " '2',\n",
       " 'Using',\n",
       " 'blood',\n",
       " 'two',\n",
       " 'cavemen',\n",
       " 'diary',\n",
       " 'todays',\n",
       " 'latest',\n",
       " 'happenings',\n",
       " 'designated',\n",
       " 'cave',\n",
       " 'diary',\n",
       " 'wall',\n",
       " '3',\n",
       " 'see',\n",
       " 'asa',\n",
       " 'pokemon',\n",
       " 'world',\n",
       " 'infj',\n",
       " 'society',\n",
       " 'everyone',\n",
       " 'becomes',\n",
       " 'optimist49142httpwwwyoutubecomwatchvZRCEqJFeFMhttpdiscovermagazinecom2012julaug20thingsyoudidntknowaboutdesertsdesertjpghttpoysterignimgscommediawikiapisigncompokemonsilverversiondddDittogifhttpwwwserebiinetpotwdpScizorjpgNot',\n",
       " 'artists',\n",
       " 'artists',\n",
       " 'draw',\n",
       " 'idea',\n",
       " 'counts',\n",
       " 'forming',\n",
       " 'something',\n",
       " 'like',\n",
       " 'signatureWelcome',\n",
       " 'robot',\n",
       " 'ranks',\n",
       " 'person',\n",
       " 'downed',\n",
       " 'selfesteem',\n",
       " 'cuz',\n",
       " 'Im',\n",
       " 'avid',\n",
       " 'signature',\n",
       " 'artist',\n",
       " 'like',\n",
       " 'proudBanned',\n",
       " 'taking',\n",
       " 'room',\n",
       " 'bed',\n",
       " 'Ya',\n",
       " 'gotta',\n",
       " 'learn',\n",
       " 'share',\n",
       " 'roacheshttpwwwyoutubecomwatchvw8IgImn57aQBanned',\n",
       " 'much',\n",
       " 'thundering',\n",
       " 'grumbling',\n",
       " 'kind',\n",
       " 'storm',\n",
       " 'yepAhh',\n",
       " 'old',\n",
       " 'high',\n",
       " 'school',\n",
       " 'music',\n",
       " 'havent',\n",
       " 'heard',\n",
       " 'ages',\n",
       " 'httpwwwyoutubecomwatchvdcCRUPCdB1wI',\n",
       " 'failed',\n",
       " 'public',\n",
       " 'speaking',\n",
       " 'class',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'Ive',\n",
       " 'sort',\n",
       " 'learned',\n",
       " 'could',\n",
       " 'better',\n",
       " 'position',\n",
       " 'big',\n",
       " 'part',\n",
       " 'failure',\n",
       " 'overloading',\n",
       " 'tooI',\n",
       " 'like',\n",
       " 'persons',\n",
       " 'mentality',\n",
       " 'Hes',\n",
       " 'confirmed',\n",
       " 'INTJ',\n",
       " 'way',\n",
       " 'httpwwwyoutubecomwatchvhGKLIGEc6MMove',\n",
       " 'Denver',\n",
       " 'area',\n",
       " 'start',\n",
       " 'new',\n",
       " 'life']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(m):\n",
    "    c = [c for c in m if c not in string.punctuation]\n",
    "    c = ''.join(c)\n",
    "    clean_mess = [word for word in c.split() if word.lower() not in stopwords.words('english')]\n",
    "    return clean_mess\n",
    "new0 = text_process(df['posts'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_posts = df.posts.head(5).apply(text_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF- freq counts get lower weights\n",
    "# Count of words per doc- columns are docs 1-n, rows are words 1-n\n",
    "\n",
    "#Sparse matrices have a lot of 0 values, save on memory\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(df['posts'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537\n"
     ]
    }
   ],
   "source": [
    "#print amount of vocab words from 1st 5 docs\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow4 = bow_transformer.transform([df['posts'][4]])\n",
    "# print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ambiguous'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See which words appear twice y the index\n",
    "bow_transformer.get_feature_names()[278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(df['posts'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non zero occurrences\n",
    "messages_bow.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparsity = {100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfTransformer().fit(messages_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_done_4 = tf.transform(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1537 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 391 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_done_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check doc freq for word 'World'\n",
    "\n",
    "tf.idf_[bow_transformer.vocabulary_['like']]\n",
    "messages_tfidf = tf.transform(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect = MultinomialNB().fit(messages_tfidf, df.type[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTJ'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_detect.predict(tf_done_4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "msg_train,msg_test,type_train,type_test = train_test_split(df.posts[:200], df.type[:200], test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer = text_process)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x1a1e22eae8>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,type_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.00      0.00      0.00         1\n",
      "        ENFP       0.00      0.00      0.00         4\n",
      "        ENTJ       0.00      0.00      0.00         2\n",
      "        ENTP       0.00      0.00      0.00         4\n",
      "        ESTP       0.00      0.00      0.00         1\n",
      "        INFJ       0.17      0.89      0.29         9\n",
      "        INFP       0.00      0.00      0.00        15\n",
      "        INTJ       0.00      0.00      0.00         6\n",
      "        INTP       0.57      0.62      0.59        13\n",
      "        ISFJ       0.00      0.00      0.00         2\n",
      "        ISTJ       0.00      0.00      0.00         1\n",
      "        ISTP       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.27        60\n",
      "   macro avg       0.06      0.13      0.07        60\n",
      "weighted avg       0.15      0.27      0.17        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(type_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perceptron- multiple inputs into processor with one output\n",
    "1. Receive inputs\n",
    "2. Place weights\n",
    "3. Sum Inputs\n",
    "4. Generate Output\n",
    "\n",
    "Weights between -1 and 1\n",
    "\n",
    "Begin by assigning random weights\n",
    "\n",
    "Activation function tells perceptron whether to fire or ont\n",
    "Sinuisoidal, sigmoid etc\n",
    "\n",
    "Ex- make activation function, sine of sum\n",
    "So if positive output 1, if negative -1\n",
    "\n",
    "Bias- what if input sum is equal to 0?\n",
    "\n",
    "Add bias input to fix this \n",
    "\n",
    "Steps to train perceptron\n",
    "\n",
    "1.  Provide percepton with inputs for which theres a known answer\n",
    "2.  Ask perceptron to guess an answer\n",
    "3.  Compute Error\n",
    "4.  Adjust weights according to error\n",
    "5.  Return to step 1 and repeat\n",
    "\n",
    "Repeat until get satisfiable error\n",
    "\n",
    "Input Layer then hidden layers then output layer\n",
    "\n",
    "Deep neural nets run better on GPU\n",
    "\n",
    "Datas are tensors\n",
    "\n",
    "2 ways To use-\n",
    "\n",
    "graph session\n",
    "sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build tensor obj\n",
    "hello =  tf.constant('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(100)\n",
    "sess = tf.Session()\n",
    "sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sess.run(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opp\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3)\n",
    "y = tf.constant(4)\n",
    "with tf.Session() as sess:\n",
    "    print('opp')\n",
    "    sess.run(x+y)\n",
    "    print(sess.run(x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32)\n",
    "y = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=<unknown> dtype=int32>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations with PlaceHolders\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "add = tf.add(x,y)\n",
    "with tf.Session() as sess:\n",
    "    print('Operations with PlaceHolders')\n",
    "    print(sess.run(add, feed_dict = {x:2,y:3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[5.0,5.0]])\n",
    "b = np.array([[2.0],[2.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = tf.constant(a)\n",
    "mat2 = tf.constant(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_multi = tf.matmul(mat1,mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    result = sess.run(matrix_multi)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST data set connecion of hand written digits\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-192-1a516c502833>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/allanporter/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/allanporter/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/allanporter/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/allanporter/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/allanporter/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/allanporter/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images\n",
    "#Series of NP Arrays\n",
    "#55K images, 784 pixels per img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mnist.train.images[1].shape\n",
    "#since only one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a29cf66a0>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAD4AAAD8CAYAAAAv4Rf7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAKwUlEQVR4nO2dbYxdRRnHf//u0q5rge5St260a9vQ0G0TC93yJhgVkMDGlBj3QymJkNSQFjEQTXQpqcEPROCDbQlGrIpCmgAtRkEM0spLCCZu3+gWFlppsYRC6bbIi4UE3fXxw8yWu9u7d8859969ZzvzS07uOXNmzsz/zpw558wzLzIzQmRSrRNQK6Lw0IjCQyMKrySSrpC0V9I+Sd3ViKNcVOnnuKQ64B/A14GDwDbgajN7uaIRlUk1cvw8YJ+ZvWZm/wEeAq6qQjxlUQ3hnwPeKDg+6N2GIel6Sdv9Zhm3I1kTWZ81YAlUxO2E+8nM1gPrASRlvd9ezxiuKjl+EJhZcPx54K0qxFMW1RC+DZgrabakycBS4LEqxFMWFS/qZjYg6UbgSaAOuM/M+iodT7lU/HGWKRHZ7/EdZrY4S8D45hYaUXhoROGhEYWHRhQeGlF4aEThoRGFh0YUHhpR+GhIuk9Sv6SXCtyaJW2R9Kr/bfLuknS3t5ntlrSomokvhyQ5/jvgihFu3cBTZjYXeMofA1wJzPXb9cAvKpPMKmBmY27ALOClguO9QKvfbwX2+v1f4gyEJ/gb4/qWcdueJP3Ftqz3+AwzO+T/uENAi3dPZDeD4bazjGkoi0obFBLZzaBitrPMZM3xw5JaAfxvv3efEHYzyC78MeBav38t8GiB+7d97X4B8P7QLZE7ElQ8DwKHgP/icnQ5cAauNn/V/zZ7vwJ+DuwHXgQWJ6w8x71yi7az0IjCQyMKD40oPDSi8NDIhfCOjg4GBwdTb+UQX1lDIwoPjSg8NKrRUT81CxcuZPPmzanDzZgxI3OcuRBeX1/P9OnTxzfOcY1tFHp7e8vKvUwkaAicCTwDvAL0ATd592ZgC67BcQvQVNDgeDewD9gNLMpjY2MS4a1DiQdOxY0pmw/cBXR7927gTr/fCTzh/4ALgJ4JKbxIIh/FDaarmP2sFsJTPc4kzQLOAXoo0342YWxnkqYCvwduNrMPpGJmMue1iFvJcWetra22fPnypEk5zu233546TGECkhTvU3Cjir5frAhTZlHv6OiwwcHB1BtlFPUxc1wua38DvGJmPys4NWQ/u4MT7Wc3SnoIOJ8E9rOPPvqInTt3jp1LlSRBbl+MK6q7gV1+66SC9jOi7Sw1sSEiLVF4aORCeC1aWYP9OsuF8IaGBubNm5c63PPPP585zvg4C40oPDSCFZ6LWr29vZ0HHnggdbhzzz03c5y5EP7hhx+yY8eO8Y0062ddJbeOjg7LAvGzND3BVm5ReGgkGX7VIGmrpF5JfZJ+4t1nS+rxQ7Ae9hNaIWmKP97nz8+qroSMJGgIFDDVPmlm7sGZhjYCS737vcBKv38DcK/fXwo8nCCOcW9sHPM57h8bx/zhKX4z4BJgmXe/H7gNN87sKr8P8AhwjyRZicdHY2Njps/ScpqkE73A+AkodwBn8knT8XtmNuC9FJqJjpuQzE1x9j6uKfroaNdvb2+np6cnfeLrs79/JQppZoPA2ZKmAX8A2ot587+JTEiSrseNRnQJKUNEFlLV6mb2HvAs7h6fJmkotYXDrI4PwfLnTwf+VeRa681scdYXkHJJUqt/xuc0kj4FXIbrJPAM0OW9jTQhXev3u4CnS93fNSNBjftF4AWcCekl4MfefQ6wFdfzYRMwxbs3+ON9/vycPNbq8V09NHLxPd7S0sI111yTOtyaNWsyxxmLemhE4aERrPBc1OpTp05l0aL0M6c899xzmePMhfCBgQH6+/vH9lhBciF8wYIF4/5ZGp/joRGFh0awwnNRq7e1tbFq1arU4VasWJE5zlzU6osXL7Zt27alDjdp0qTMtXoucvzNN99k9erV4xpnLnI8189xSXWSXpD0uD+e0LazNLX6Tbhm5SHuBNaYm77wXdzkV/jfd83sTGCN95c/kjTF4gwGT+HsZY/jrCVHgXp//kLgSb//JHCh36/3/pS35uWkOb4W+CHwP398BgltZ8CQ7WwYtR5+lcSS8g2g38wKuyWVso8lHn41ZELq6OhgYGAg9VYOSR5nFwFLJHXirCSn4UrANEn1PleL2c4OlrKd1Zw09wXwVeBxv7+J4R0DbvD732V4x4CNeTQhlSM82s7KJdcvMCcbUXho5OLrLNhu242NjZkMCuUQa/XQiMJDIwoPjVw8zhoaGpg1a1bqcHv27MkcZy6Ez549e9xfYOJzPDSi8NCIwkMjkXBJByS9KGnXkAFgoi/9leY5/jUzKxxJNLT01x2Suv3xjxi+9Nf5uCFZ55e6cEtLC8uWLSvlpShr165NHeY4CZuVDwDTR7gFMXWhAZsl7fDDpiCQqQsvMrO3JLUAWySVeklOPXVhbpf9MrO3/G8/bsDdeUzwpb+SWEs/LenUoX3gctwwrIm99FeCim0O0Ou3PuBW7x6nLiyX+HU2jkThoRGs8Fy0uS1YsIBNmzalDjd//vzMccZaPTRyU9Q3btyYKVxWYlEPjSg8NHJRubW1tXHLLbekDrdy5crMccbKLTRyITwu5pieWNTTEoWXQtI0SY9I2iPpFUkXhmI7Wwf8xcy6/MC6RmAVFbKdNTc309nZmTrxGzZsSB3mOAmafk8D/smIsWMEYDubAxwBfuuHWP7aGxYqZjtra2vL5aoZ9cAi4Htm1iNpHa5Yj0Zq29nMmTNt3bp1CZJSQRIU9c8CBwqOvwz8mZO9qJvZ28Abks7yTpcCL3Oy2858jpwNbMfN2/hHoIloOyufYF9Zg10LaWBggKNHR52huCrkQngt1iaO93hoROGhkZvKrampKXW4I0eOZI8zc8gKsnDhQrZu3Zo6XF1dXeY4Y1GvJfv376erq2tsjxUkPsdDIwoPjSi8lgT7Pd7X11dWD6ZMJGgPOwvYVbB9ANwMNANbcG1uW4Cmgja3u3FzPe0GFuWxzS2dZ6gD3ga+ANwFdHv3buBOv98JPOH/gAuAnpNB+OXA3/z+yd2uPoKlwIN+vywTUq1JMz3pZGAJbq62kl6LuJ3wSlrrcWdpcvxKYKeZHfbHZQ2/srwv+1XA1XxSzCEQE1Ij8A5weoFbNCGVS/wsHUei8NCIwkMjCg+NKDw0ovDQiMJDIwoPjSg8NIIVngvbGXAMZ3goxnTcekrFOGsU9zHJi/C9o7WdSdpe6lzWCIMt6lF4jVlfhXMlyUW7ei3IS46PO1H4eFJsJLKkKyTt9aOQu72/QT/7735JxyT9W9LRkatjSrpO0hHvd5ek74yZiKxGt3I2TuxGchfOyDgHmIybH3I+7sWmzp9bjavMeoEfAA8XXO864J40aajVC8xVuEXjAO4H/g7sMbPXACQ95P2Amwp1H3AxcBtuZHMDcKkkWcbauVb3+MhuJGdQvPtIA7ABOAeY5/0cxPW5Gbk65rf8QP1HJBV2TChK1XJc0l9xA3JHcmvCSxjQhltMsgv4Jq631dC5wt8/AQ+a2ceSVuBK0SWlLl414WZ22WjnJB2W1Gpmh3w3knco0n3E3LTHB4FpuDHsX8GJfZuC1THN7J2CsL8iweq4tSrqI7uRbATmyq13PBnXu+pZSVOAbbivsCm4Ir8U+Bh4euj+HuqL41nC8KWEi1OjWv2EbiS4joGv43pO3gp8CTiMq8wO+P1juNKxFdd7com/3k9xswT3As8A8yZEV5BaEN/cQiMKD40oPDSCFf5/x/h+rv2JD14AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Change 784 pixels to 28 * 28 pixelated image\n",
    "i1 = mnist.train.images[1].reshape(28,28)\n",
    "plt.imshow(i1.reshape(784,1), cmap='gist_gray',aspect = 0.02)\n",
    "#show image\n",
    "#aspect will condense it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the 784 * 1 image into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#55k images too much, so break into batches\n",
    "#set shape[0] to none (since we dont know amount of images to put in yet, but w know 784 pixes,\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])\n",
    "W = tf.Variable(tf.zeros([784,10])) #10 possible labels 0-9 images\n",
    "# w for weights var\n",
    "b = tf.Variable(tf.zeros([10])) #Bias- 10 size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_4:0' shape=(784, 10) dtype=float32_ref>\n",
      "Tensor(\"Placeholder_10:0\", shape=(?, 784), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(x)\n",
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=y))\n",
    "# Use softmax activ func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce error between ytrue and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "#Lower learning rate slower but more accurate\n",
    "#Guess and check method to find a good rate\n",
    "train = optimizer.minimize(cross_entropy)\n",
    "#This is defining the error, grab optimizer and actually minize the error in cross_entropy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the TF session\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9178\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1000):\n",
    "        #grab batch of 100 numbers and grab x and y values\n",
    "        batch_x,batch_y = mnist.train.next_batch(100)\n",
    "        #feed_dict are actual values for x,y\n",
    "        sess.run(train,feed_dict = {x:batch_x,y_true:batch_y})\n",
    "    matches = tf.equal(tf.argmax(y,1),tf.argmax(y_true,1))\n",
    "    acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "    print(sess.run(acc,feed_dict={x:mnist.test.images,y_true:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-245-2dc145fa946a>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-245-2dc145fa946a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    1. Read in data\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#SKlearn approach\n",
    "\n",
    "\n",
    "#Estimator\n",
    "1. Read in data\n",
    "2. Normalize if necessary\n",
    "3. Train/Test Split\n",
    "4. Create Estimator feat columns\n",
    "5. Create input estimator func\n",
    "6. train estimator model\n",
    "7. predict with new test input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,:-1]\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,y_train,X_test,y_test = train_test_split(x,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to create features cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.feature_column.numeric_column\n",
    "feat_cols = []\n",
    "for col in x.columns:\n",
    "    feat_cols.append(tf.feature_column.numeric_column(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='sepal_length', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='sepal_width', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='petal_length', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='petal_width', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='species', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index for x and y are mismatched.\nIndex for x: Int64Index([ 44, 130,  46,  27,  73, 147, 122, 110,  92,  21,\n            ...\n             28,   7, 131, 116,   4, 101,  22,  97,  34,  72],\n           dtype='int64', length=105)\nIndex for y: Int64Index([ 85, 111,  15, 140,  20,  12,  45,  69,  11,  90,  42,  59, 146,\n             78,  68,  96,  51, 120, 113,  83,  40,  77,  49,  25, 128, 107,\n             64, 102,  91,  10, 143,  75,  58,  94, 118,   0,  14, 112,  79,\n            132,  67,  33, 141,  56,  80],\n           dtype='int64')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-270-50722f94a383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#put batches in neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#epoch is when go through training data once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minput_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/pandas_io.py\u001b[0m in \u001b[0;36mpandas_input_fn\u001b[0;34m(x, y, batch_size, num_epochs, shuffle, queue_capacity, num_threads, target_column)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       raise ValueError('Index for x and y are mismatched.\\nIndex for x: %s\\n'\n\u001b[0;32m--> 112\u001b[0;31m                        'Index for y: %s\\n' % (x.index, y.index))\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       y_columns = [(column, _get_unique_target_key(x, column))\n",
      "\u001b[0;31mValueError\u001b[0m: Index for x and y are mismatched.\nIndex for x: Int64Index([ 44, 130,  46,  27,  73, 147, 122, 110,  92,  21,\n            ...\n             28,   7, 131, 116,   4, 101,  22,  97,  34,  72],\n           dtype='int64', length=105)\nIndex for y: Int64Index([ 85, 111,  15, 140,  20,  12,  45,  69,  11,  90,  42,  59, 146,\n             78,  68,  96,  51, 120, 113,  83,  40,  77,  49,  25, 128, 107,\n             64, 102,  91,  10, 143,  75,  58,  94, 118,   0,  14, 112,  79,\n            132,  67,  33, 141,  56,  80],\n           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "#put batches in neural network\n",
    "#epoch is when go through training data once\n",
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,batch_size = 10, num_epochs = 5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
